frameworkVersion: "3.32.2"
service: foxscript-task

useDotenv: true

plugins:
  - serverless-dotenv-plugin
  - serverless-offline
  - serverless-python-requirements
  - serverless-fargate
  - serverless-vpc-plugin
  - serverless-lift

custom:
  stage: ${opt:stage, self:provider.stage}

  bucket_name: foxscript-${self:custom.stage}

  serverless-offline:
    httpPort: 3007

  # Lambda Layers
  pythonRequirements:
    dockerizePip: true
    pipCmdExtraArgs: ['--platform manylinux2014_x86_64', '--only-binary=:all:']
    noDeploy:
      - unittest-parametrize
    layer:
      name: ${self:service}
      compatibleRuntimes:
        - python3.11

  # VPC
  vpcConfig:
    vpcEnabled:
      foxscript-task: true

    enabled: true
 
    cidrBlock: '10.0.0.0/16'
 
    # if createNatGateway is a boolean "true", a NAT Gateway and EIP will be provisioned in each zone
    # if createNatGateway is a number, that number of NAT Gateways will be provisioned
    createNatGateway: false
 
    # When enabled, the DB subnet will only be accessible from the Application subnet
    # Both the Public and Application subnets will be accessible from 0.0.0.0/0
    createNetworkAcl: true
 
    # Whether to create the DB subnet
    createDbSubnet: false
 
    # Whether to enable VPC flow logging to an S3 bucket
    createFlowLogs: false
 
    # Whether to create a bastion host
    createBastionHost: false
    bastionHostKeyName: MyKey # required if creating a bastion host
 
    # Whether to create a NAT instance
    createNatInstance: true
 
    # Whether to create AWS Systems Manager (SSM) Parameters
    createParameters: true
 
    # Optionally specify AZs (defaults to auto-discover all availabile AZs)
    zones:
      - us-east-1a
      - us-east-1b
      - us-east-1c
 
    # By default, S3 and DynamoDB endpoints will be available within the VPC
    # see https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html
    # for a list of available service endpoints to provision within the VPC
    # (varies per region)
    services:
    #  - lambda
    #  - sqs
    #  - cloudformation
    #  - ecs-agent
    #  - ecs-telemetry
    #  - lambda
    #  - ecr.dkr
    #  - ecr.api
    #  - logs
    #  - ssm
 
    # Whether to export stack outputs so it may be consumed by other stacks
    exportOutputs: true

package:
  individually: true
  patterns:
    - '!node_modules/**'
    - '!package-lock.json'
    - '!package.json'
    - batch_api.py
    - run_batch_workflow.py
    - run_batch_upload_to_s3.py
    - ../utils/**

provider:
  name: aws
  runtime: python3.11
  stage: dev
  region: us-east-1
  lambdaHashingVersion: 20201221
  apiGateway:
    restApiId:
      'Fn::ImportValue': foxscript-${self:custom.stage}-ApiGateway-restApiId
    restApiRootResourceId:
      'Fn::ImportValue': foxscript-${self:custom.stage}-ApiGateway-rootResourceId
  ecr:
    images:
      task_image:
        path: ../..
        file: task.Dockerfile
      data_image:
        path: ../..
        file: data.Dockerfile

  environment:
    SERVICE: ${self:service}
    STAGE: ${self:custom.stage}
    WEAVIATE_URL: ${env:WEAVIATE_URL}
    WEAVIATE_API_KEY: ${env:WEAVIATE_API_KEY}
    OPENAI_API_KEY: ${env:OPENAI_API_KEY}
    OPENAI_ORGANIZATION: ${env:OPENAI_ORGANIZATION}
    AZURE_OPENAI_API_KEY: ${env:AZURE_OPENAI_API_KEY}
    AZURE_OPENAI_ENDPOINT: ${env:AZURE_OPENAI_ENDPOINT}
    SERPER_API_KEY: ${env:SERPER_API_KEY}
    SCRAPER_API_KEY: ${env:SCRAPER_API_KEY}
    BUBBLE_API_ROOT: ${env:BUBBLE_API_ROOT}
    BUBBLE_WF_API_ROOT: ${env:BUBBLE_WF_API_ROOT}
    BUBBLE_API_KEY: ${env:BUBBLE_API_KEY}
    GOOG_CLIENT_ID: ${env:GOOG_CLIENT_ID}
    GOOG_CLIENT_SECRET: ${env:GOOG_CLIENT_SECRET}
    ORIGINALITY_PLAG: ${env:ORIGINALITY_PLAG}
    QUEUE_URL: ${construct:ecs-task.queueUrl}
    DLQ_URL: ${construct:ecs-task.dlqUrl}

  iam:
    role:
      statements:
        - Effect: 'Allow'
          Action:
            # Lambda Actions
            - 'lambda:InvokeFunction'
            - 'lambda:InvokeAsync'
            # S3 Actions
            - 's3:PutObject'
            - 's3:GetObject'
            - 's3:DeleteObject'
            - 's3:ListBucket'
            # ECS Actions
            - 'ecs:*'
            # ECR Actions
            - 'ecr.dkr:*'
            - 'ecr.api:*'
            - 'ecr:*'
            - 'logs:*'
            - 'ssm:*'
            - 'iam:*'
            - 'cloudformation:*'
            - 'sqs:*'
            # Secrets Manager
            - 'kms:GenerateDataKey'
            - 'kms:Decrypt'
            - 'secretsmanager:*'
          Resource: '*'
        - Effect: Allow
          Action:
            - xray:PutTraceSegments
            - xray:PutTelemetryRecords
          Resource: '*'

constructs:
  ecs-task:
    type: queue
    alarm: ledger@foxscript.ai
    fifo: true

fargate:
  tasks:
    run_batch_workflow:
      name: 'run_batch_workflow-${self:custom.stage}'
      image: task_image
      memory: '4GB'
      cpu: 1024
      command:
        - run_batch_workflow.py
      environment:
        STAGE: ${self:custom.stage}
        WEAVIATE_URL: ${env:WEAVIATE_URL}
        WEAVIATE_API_KEY: ${env:WEAVIATE_API_KEY}
        AZURE_OPENAI_API_KEY: ${env:AZURE_OPENAI_API_KEY}
        AZURE_OPENAI_ENDPOINT: ${env:AZURE_OPENAI_ENDPOINT}
        SERPER_API_KEY: ${env:SERPER_API_KEY}
        SCRAPER_API_KEY: ${env:SCRAPER_API_KEY}
        BUBBLE_API_ROOT: ${env:BUBBLE_API_ROOT}
        BUBBLE_WF_API_ROOT: ${env:BUBBLE_WF_API_ROOT}
        BUBBLE_API_KEY: ${env:BUBBLE_API_KEY}
        BUCKET: ${self:custom.bucket_name}
        GOOG_CLIENT_ID: ${env:GOOG_CLIENT_ID}
        GOOG_CLIENT_SECRET: ${env:GOOG_CLIENT_SECRET}
        ORIGINALITY_PLAG: ${env:ORIGINALITY_PLAG}
      schedule: 'rate(365 days)'
      vpc:
        assignPublicIp: true

    run_batch_upload_to_s3:
      name: 'run_batch_upload_to_s3-${self:custom.stage}'
      image: task_image
      memory: '4GB'
      cpu: 1024
      command:
        - run_batch_upload_to_s3.py
      environment:
        STAGE: ${self:custom.stage}
        WEAVIATE_URL: ${env:WEAVIATE_URL}
        WEAVIATE_API_KEY: ${env:WEAVIATE_API_KEY}
        AZURE_OPENAI_API_KEY: ${env:AZURE_OPENAI_API_KEY}
        AZURE_OPENAI_ENDPOINT: ${env:AZURE_OPENAI_ENDPOINT}
        SERPER_API_KEY: ${env:SERPER_API_KEY}
        SCRAPER_API_KEY: ${env:SCRAPER_API_KEY}
        BUBBLE_API_ROOT: ${env:BUBBLE_API_ROOT}
        BUBBLE_WF_API_ROOT: ${env:BUBBLE_WF_API_ROOT}
        BUBBLE_API_KEY: ${env:BUBBLE_API_KEY}
        BUCKET: ${self:custom.bucket_name}
        GOOG_CLIENT_ID: ${env:GOOG_CLIENT_ID}
        GOOG_CLIENT_SECRET: ${env:GOOG_CLIENT_SECRET}
        ORIGINALITY_PLAG: ${env:ORIGINALITY_PLAG}
      schedule: 'rate(365 days)'
      vpc:
        assignPublicIp: true

    run_task:
      name: 'run_task-${self:custom.stage}'
      image: task_image
      memory: '4GB'
      cpu: 1024
      command:
        - run_task_ecs.py
      environment:
        STAGE: ${self:custom.stage}
        WEAVIATE_URL: ${env:WEAVIATE_URL}
        WEAVIATE_API_KEY: ${env:WEAVIATE_API_KEY}
        AZURE_OPENAI_API_KEY: ${env:AZURE_OPENAI_API_KEY}
        AZURE_OPENAI_ENDPOINT: ${env:AZURE_OPENAI_ENDPOINT}
        SERPER_API_KEY: ${env:SERPER_API_KEY}
        SCRAPER_API_KEY: ${env:SCRAPER_API_KEY}
        BUBBLE_API_ROOT: ${env:BUBBLE_API_ROOT}
        BUBBLE_WF_API_ROOT: ${env:BUBBLE_WF_API_ROOT}
        BUBBLE_API_KEY: ${env:BUBBLE_API_KEY}
        BUCKET: ${self:custom.bucket_name}
        GOOG_CLIENT_ID: ${env:GOOG_CLIENT_ID}
        GOOG_CLIENT_SECRET: ${env:GOOG_CLIENT_SECRET}
        ORIGINALITY_PLAG: ${env:ORIGINALITY_PLAG}
      schedule: 'rate(365 days)'
      vpc:
        assignPublicIp: true

functions:
  task:
    handler: task_api.handler
    memorySize: 256
    timeout: 900
    maximumRetryAttempts: 0
    layers:
      - Ref: PythonRequirementsLambdaLayer
    events:
      - http:
          path: task_api
          method: post
          cors: true

  batch_workflow:
    handler: batch_api.batch_workflow
    memorySize: 512
    timeout: 300
    maximumRetryAttempts: 0
    layers:
      - Ref: PythonRequirementsLambdaLayer
    events:
      - http:
          path: batch_workflow
          method: post
          cors: true

  batch_upload_to_s3:
    handler: batch_api.batch_upload_to_s3
    memorySize: 256
    timeout: 300
    maximumRetryAttempts: 0
    layers:
      - Ref: PythonRequirementsLambdaLayer
    events:
      - http:
          path: batch_upload_to_s3
          method: post
          cors: true